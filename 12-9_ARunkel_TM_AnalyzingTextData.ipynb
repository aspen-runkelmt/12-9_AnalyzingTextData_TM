{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing a Text Data Set\n",
    "This notebook creates a text data set using a Twitter API and analyzes the data using some parsing.\n",
    "My goal for this assignment was to utilize methods that I am knowledgeable of on a data set that interests me - aka National Parks.\n",
    "\n",
    "#### A New Angle Soundcloud data note:\n",
    "Originally, I planned to complete this assignment using some A New Angle text data, but I wasn't able to compile the dataset and do the analysis in the time remaining with my current skillset. When I have more time and improve my coding skills, I hope to continue that project. Sorry John, I know that you really wanted me to stick with the pod data. Trust me, I did to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Creates a text data set using Twitter API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports for creating text data set\n",
    "from datetime import datetime\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "from twitter_functions import * # these are the functions John wrote for ADA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#My Twitter credentials\n",
    "auth =  { \"consumer_key\": \"OnllVAvDyo1P59VygbJaRRctU\", \n",
    "          \"consumer_secret\": \"2d0DspkrPRS9aCwemPx3U13gd7IpZr0ASSqdCy1ZaP3ALlqvx3\",\n",
    "          \"access_key\": \"1169449721665974273-GP9tiIGcfZ7weAoKEub4Kxdk4D5hdm\",\n",
    "          \"access_secret\": \"PHbUPHSdxn1og9OrCcIlE5bacFRLTI1BPooqKxvwvBOQ3\"}\n",
    "\n",
    "api = initialize_twitter(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells pull all the Twitter followers of Glacier National Park (263.5K) and Grand Canyon National Park (133.2K). \n",
    "\n",
    "Then it pulls the following Twitter data of those users:\n",
    "* screen_name\n",
    "* name\n",
    "* id\n",
    "* location\n",
    "* followers_count\n",
    "* friends_count\n",
    "* description\n",
    "\n",
    "and then writes this to a text file for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#twitter_user = ['GlacierNPS'] # group1 - Glacier National Park (263.5K Followers) - 0.88hrs est. time\n",
    "twitter_user = ['GrandCanyonNPS'] # group2 - Grand Canyon NPS (133.2K Followers) - 0.44hrs est. time\n",
    "\n",
    "ofile_name = (datetime.today().strftime(\"%Y%m%d\") + \"_\" + \n",
    "             starting_user[0] + \"_\" +\n",
    "             \"followers.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looks up the full information on my starting user(s) (GNP or GCNP)\n",
    "starting_user_id = []\n",
    "\n",
    "# All records will be a dictionary with the twitter ID as the key and \n",
    "# a UserRecord as the value. This is a named tuple John created in twitter_functions.py. \n",
    "all_records = lookup_users_from_handles(api, starting_user)\n",
    "\n",
    "# Creates list of IDs that we are getting followers from \n",
    "for id in all_records : #access the keys, which are ids.\n",
    "    starting_user_id.append(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was super helpful as I originally planned to pull the U.S. Soccer WNT (@USWNT) 2.5M Followers and the U.S. Soccer MNT (@USMNT) 2.2M. A complete pull WITHOUT run limits would have take 7-8 hours each. I simply didn't have the time and scaled back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimates time it will take to pull these followers\n",
    "total_followers = 0\n",
    "for id, rec in all_records.items() :\n",
    "    total_followers += rec.followers_count\n",
    "    \n",
    "print(\"Ooh, {fol:,d} followers. A complete run with no limits run is \".format(fol=total_followers) + \n",
    "      \"going to take {min:.2f} minutes ({hour:.2f} hours)\".format(min=total_followers/5000,\n",
    "                                                                  hour=total_followers/(60*5000)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we go, go, go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This pulls all the followers of our starting_user (GNP or GCNP)\n",
    "\n",
    "followers_of_starting = gather_followers(api,\n",
    "                                         starting_user_id,\n",
    "                                         follower_limit=None)  \n",
    "\n",
    "# followers_of_starting will be a dictionary with the key being the id(s) in starting_user_id\n",
    "# and the value is a list of all the followers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This \"hydrates\" these user records with screen_name, name, id, location, followers_count, friends_count, description\n",
    "for start_id, list_of_followers in followers_of_starting.items() :\n",
    "\n",
    "    ids_to_hydrate = {id for id in list_of_followers if id not in all_records} \n",
    "    \n",
    "    these_records = lookup_users_from_ids(api,ids=ids_to_hydrate)\n",
    "\n",
    "    for id, rec in these_records.items() :\n",
    "        all_records[id] = rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This writes out all the records to a text file.\n",
    "with open(ofile_name,'w') as ofile :\n",
    "    write_user_rec_headers(ofile)\n",
    "    for id, rec in all_records.items() :\n",
    "        write_user_rec(ofile, rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### END Step 1 - Create text data set using Twitter API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Analysis of a text data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports for analyzing text data set\n",
    "import nltk\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing - twitter descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reads in text files (descriptions) created in step 1\n",
    "\n",
    "file_name = \"20191205_GlacierNPS_followers.txt\"\n",
    "#file_name = \"20191205_GrandCanyonNPS_followers.txt\"\n",
    "\n",
    "descs = []\n",
    "with open(file_name,'r') as ifile :\n",
    "    next(ifile)\n",
    "    \n",
    "    for idx, line in enumerate(ifile.readlines()) :\n",
    "        line = line.strip().split(\"\\t\")\n",
    "        \n",
    "        # spot 6 has the description\n",
    "        if len(line) >= 7 : # sometimes we don't have descriptions\n",
    "            descs.extend(line[6].split())\n",
    "        \n",
    "        # for now we'll just add on to a big list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_name,'r') as ifile :\n",
    "    print(ifile.readline()) #prints \"headers\"\n",
    "    print(ifile.readline()) #prints corresponding info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cast words to lowercase, removes stopwords, and includes only alphanumeric words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stopwords_sp = set(nltk.corpus.stopwords.words(\"spanish\"))\n",
    "\n",
    "def clean_list(text) :\n",
    "    ''' takes a list of text and returns a new list with \n",
    "        * words cast to lowercase\n",
    "        * stopwords removed\n",
    "        * only alphanumeric words\n",
    "    '''\n",
    "    text_clean = [w.lower() for w in text if w.isalpha()]\n",
    "    text_clean = [w for w in text_clean if w not in stopwords]\n",
    "    text_clean = [w for w in text_clean if w not in stopwords_sp]\n",
    "    return(text_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean list\n",
    "descs_clean_gnp = clean_list(descs) #Glacier National Park\n",
    "#descs_clean_gcnp = clean_list(descs) #Grand Canyon National Park"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(descs_clean_gnp.count(\"montana\")) #number of times word was used\n",
    "print(100 * descs_clean_gnp.count(\"montana\")/len(descs_clean_gnp)) #percent\n",
    "\n",
    "#print(descs_clean_gcnp.count(\"life\")) #number of times word was used\n",
    "#print(100 * descs_clean_gcnp.count(\"life\")/len(descs_clean_gcnp)) #percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frequency Distribution\n",
    "fd_gnp = FreqDist(descs_clean_gnp)\n",
    "fd_gnp.most_common(50)\n",
    "\n",
    "#fd_gcnp = FreqDist(descs_clean_gcnp)\n",
    "#fd_gcnp.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glacier National Park\n",
    "Number of times word was used:\n",
    "* montana = 1282 (0.1951706677668487%)\n",
    "* mt = 294 (0.04475832786540836%)\n",
    "* arizona = 121 (0.018420944461613645%)\n",
    "* az = 70 (0.010656744729859132%)\n",
    "* love = 6547 (0.9967101106626821%)\n",
    "* life = 3494 (0.5319238012303973%)\n",
    "\n",
    "Frequency distribution: (top 50)\n",
    "\n",
    "[('love', 6547),\n",
    " ('life', 3494),\n",
    " ('news', 3411),\n",
    " ('music', 2678),\n",
    " ('politics', 2592),\n",
    " ('science', 2409),\n",
    " ('world', 2385),\n",
    " ('lover', 2324),\n",
    " ('sports', 2311),\n",
    " ('like', 2199),\n",
    " ('travel', 2124),\n",
    " ('national', 1937),\n",
    " ('government', 1933),\n",
    " ('one', 1831),\n",
    " ('us', 1817),\n",
    " ('new', 1717),\n",
    " ('living', 1653),\n",
    " ('good', 1652),\n",
    " ('things', 1651),\n",
    " ('proud', 1617),\n",
    " ('mom', 1550),\n",
    " ('live', 1538),\n",
    " ('people', 1521),\n",
    " ('business', 1500),\n",
    " ('make', 1477),\n",
    " ('great', 1458),\n",
    " ('fan', 1420),\n",
    " ('time', 1416),\n",
    " ('nature', 1392),\n",
    " ('student', 1365),\n",
    " ('follow', 1356),\n",
    " ('family', 1318),\n",
    " ('best', 1313),\n",
    " ('outdoor', 1295),\n",
    " ('montana', 1282),\n",
    " ('go', 1252),\n",
    " ('university', 1227),\n",
    " ('state', 1188),\n",
    " ('art', 1170),\n",
    " ('get', 1161),\n",
    " ('retired', 1156),\n",
    " ('twitter', 1153),\n",
    " ('tweets', 1147),\n",
    " ('technology', 1131),\n",
    " ('dog', 1127),\n",
    " ('social', 1124),\n",
    " ('country', 1114),\n",
    " ('always', 1100),\n",
    " ('former', 1068),\n",
    " ('college', 1058)]\n",
    "\n",
    "### Grand Canyon National Park\n",
    "Number of times word was used:\n",
    "* arizona = 545 (0.11823690336465345%)\n",
    "* az = 274 (0.0594438743521377%)\n",
    "* montana = 109 (0.023647380672930693%)\n",
    "* mt = 36 (0.007810144075463348%)\n",
    "* love = 5031 (1.0914676345460028%)\n",
    "* life = 2711 (0.5881472385716983%)\n",
    "\n",
    "Frequency distribution: (top 50)\n",
    "\n",
    "[('love', 5031),\n",
    " ('life', 2711),\n",
    " ('lover', 2013),\n",
    " ('travel', 1980),\n",
    " ('world', 1753),\n",
    " ('music', 1708),\n",
    " ('national', 1691),\n",
    " ('like', 1638),\n",
    " ('news', 1575),\n",
    " ('new', 1421),\n",
    " ('one', 1415),\n",
    " ('us', 1328),\n",
    " ('sports', 1324),\n",
    " ('living', 1307),\n",
    " ('things', 1305),\n",
    " ('fan', 1287),\n",
    " ('science', 1281),\n",
    " ('proud', 1270),\n",
    " ('mom', 1269),\n",
    " ('good', 1183),\n",
    " ('nature', 1177),\n",
    " ('live', 1173),\n",
    " ('make', 1052),\n",
    " ('time', 1049),\n",
    " ('family', 1040),\n",
    " ('great', 1032),\n",
    " ('tweets', 1030),\n",
    " ('people', 1030),\n",
    " ('follow', 1029),\n",
    " ('outdoor', 1022),\n",
    " ('go', 972),\n",
    " ('best', 961),\n",
    " ('student', 938),\n",
    " ('photographer', 928),\n",
    " ('twitter', 913),\n",
    " ('opinions', 912),\n",
    " ('park', 900),\n",
    " ('state', 892),\n",
    " ('always', 885),\n",
    " ('university', 882),\n",
    " ('loves', 881),\n",
    " ('former', 881),\n",
    " ('social', 878),\n",
    " ('dog', 876),\n",
    " ('art', 870),\n",
    " ('get', 851),\n",
    " ('views', 830),\n",
    " ('public', 824),\n",
    " ('teacher', 814),\n",
    " ('business', 808)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing - twitter location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reads in text files (descriptions) created in step 1\n",
    "\n",
    "file_name = \"20191205_GlacierNPS_followers.txt\"\n",
    "#file_name = \"20191205_GrandCanyonNPS_followers.txt\"\n",
    "\n",
    "location = []\n",
    "with open(file_name,'r') as ifile :\n",
    "    next(ifile)\n",
    "    \n",
    "    for idx, line in enumerate(ifile.readlines()) :\n",
    "        line = line.strip().split(\"\\t\")\n",
    "        \n",
    "        # spot 3 has the location\n",
    "        if len(line) >= 7 : # sometimes we don't have descriptions\n",
    "            location.extend(line[3].split())\n",
    "        \n",
    "        # for now we'll just add on to a big list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_name,'r') as ifile :\n",
    "    print(ifile.readline()) #prints \"headers\"\n",
    "    print(ifile.readline()) #prints corresponding info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_list(text) :\n",
    "    ''' takes a list of text and returns a new list with \n",
    "        * words cast to lowercase\n",
    "        * only alphanumeric words\n",
    "    '''\n",
    "    text_clean = [w.lower() for w in text if w.isalpha()]\n",
    "    return(text_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean list\n",
    "loc_clean_gnp = clean_list(location) #Glacier National Park\n",
    "#loc_clean_gcnp = clean_list(location) #Grand Canyon National Park"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loc_clean_gnp.count(\"grand canyon\")) #number of times word was used\n",
    "print(100 * loc_clean_gnp.count(\"grand canyon\")/len(loc_clean_gnp)) #percent\n",
    "\n",
    "#print(loc_clean_gcnp.count(\"canyon\")) #number of times word was used\n",
    "#print(100 * loc_clean_gcnp.count(\"canyon\")/len(loc_clean_gcnp)) #percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frequency Distribution\n",
    "fd_gnp = FreqDist(loc_clean_gnp)\n",
    "fd_gnp.most_common(50)\n",
    "\n",
    "#fd_gcnp = FreqDist(loc_clean_gcnp)\n",
    "#fd_gcnp.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glacier National Park\n",
    "Number of times location was:\n",
    "* montana = 1559 (1.1696213547801426%)\n",
    "* mt = 3698 (2.774380865924931%)\n",
    "* missoula = 103 (0.07727453466475606%)\n",
    "* arizona = 224 (0.1680533569408287%)\n",
    "* az = 681 (0.5109122146281444%)\n",
    "\n",
    "Frequency distribution: (top 50)\n",
    "\n",
    "[('usa', 11209),\n",
    " ('ca', 4013),\n",
    " ('mt', 3698),\n",
    " ('united', 3185),\n",
    " ('states', 2993),\n",
    " ('new', 2309),\n",
    " ('tx', 1925),\n",
    " ('san', 1669),\n",
    " ('pakistan', 1655),\n",
    " ('wa', 1636),\n",
    " ('ny', 1580),\n",
    " ('montana', 1559),\n",
    " ('the', 1491),\n",
    " ('fl', 1362),\n",
    " ('of', 1311),\n",
    " ('co', 1186),\n",
    " ('los', 1144),\n",
    " ('il', 1119),\n",
    " ('in', 1077),\n",
    " ('dc', 1012),\n",
    " ('pa', 1004),\n",
    " ('nc', 908),\n",
    " ('oh', 831),\n",
    " ('mn', 819),\n",
    " ('north', 792),\n",
    " ('ga', 790),\n",
    " ('california', 769),\n",
    " ('or', 755),\n",
    " ('tn', 725),\n",
    " ('ma', 689),\n",
    " ('az', 681),\n",
    " ('va', 665),\n",
    " ('republic', 654),\n",
    " ('south', 650),\n",
    " ('mi', 617),\n",
    " ('texas', 614),\n",
    " ('mo', 586),\n",
    " ('colorado', 564),\n",
    " ('wi', 539),\n",
    " ('west', 518),\n",
    " ('bangladesh', 515),\n",
    " ('earth', 478),\n",
    " ('canada', 475),\n",
    " ('florida', 469),\n",
    " ('la', 465),\n",
    " ('ut', 462),\n",
    " ('iraq', 460),\n",
    " ('lake', 459),\n",
    " ('york', 457),\n",
    " ('s', 448)]\n",
    "\n",
    "### Grand Canyon National Park\n",
    "Number of times location was:\n",
    "* montana = 125 (0.13259645065820877%)\n",
    "* mt = 282 (0.299137592684919%)\n",
    "* missoula = 6 (0.006364629631594021%)\n",
    "* arizona = 834 (0.884683518791569%)\n",
    "* az = 2131 (2.2605042908211432%)\n",
    "\n",
    "Frequency distribution: (top 50)\n",
    "\n",
    "[('usa', 7645),\n",
    " ('ca', 3809),\n",
    " ('az', 2131),\n",
    " ('united', 2097),\n",
    " ('states', 1911),\n",
    " ('new', 1817),\n",
    " ('san', 1565),\n",
    " ('tx', 1562),\n",
    " ('the', 1389),\n",
    " ('los', 1204),\n",
    " ('ny', 1170),\n",
    " ('fl', 1011),\n",
    " ('co', 965),\n",
    " ('california', 918),\n",
    " ('wa', 916),\n",
    " ('in', 846),\n",
    " ('arizona', 834),\n",
    " ('il', 817),\n",
    " ('pa', 789),\n",
    " ('dc', 745),\n",
    " ('of', 739),\n",
    " ('nc', 629),\n",
    " ('oh', 612),\n",
    " ('ga', 606),\n",
    " ('texas', 601),\n",
    " ('north', 595),\n",
    " ('las', 587),\n",
    " ('south', 578),\n",
    " ('tn', 575),\n",
    " ('ma', 549),\n",
    " ('va', 547),\n",
    " ('england', 522),\n",
    " ('ut', 515),\n",
    " ('nv', 511),\n",
    " ('colorado', 498),\n",
    " ('or', 490),\n",
    " ('mo', 455),\n",
    " ('york', 440),\n",
    " ('lake', 435),\n",
    " ('mi', 430),\n",
    " ('earth', 429),\n",
    " ('mn', 428),\n",
    " ('florida', 398),\n",
    " ('canada', 384),\n",
    " ('city', 377),\n",
    " ('angeles', 373),\n",
    " ('west', 367),\n",
    " ('la', 361),\n",
    " ('ohio', 359),\n",
    " ('utah', 346)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### END - Analysis of a text data set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
